{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Algorithmes des k plus proches voisins\n\n### I. Introduction\n\nNous allons maintenant travailler sur un **algorithme d'apprentissage automatique**, souvent appelé\nalgorithme de **machine learning**. L'idée est d'utiliser un grand nombre de données afin \"d'apprendre\nà la machine\" à résoudre un certain type de problème.\n\nCette idée d'apprentissage automatique ne date pas d'hier, puisque le terme de machine learning a\nété utilisé pour la première fois par l'informaticien américain Arthur Samuel en 1959.\n\nCe qui est important dans les algorithmes de machine learning, c'est **la qualité et la quantité des\ndonnées**. Avec le développement d'internet, il est relativement simple de trouver des données sur\nn'importe quel sujet, on parle de **Big Data**.\n\nÀ noter l'importance des stratégies mises en place par les GAFAM (Google, Apple, Facebook,\nAmazon et Microsoft) afin de récupérer un grand nombre de données concernant leurs clients. Ces\ndonnées sont très souvent utilisées pour nourrir des algorithmes de machine learning.\n\nNous allons étudier un algorithme d'apprentissage assez simple à appréhender : **l'algorithme des k\nplus proches voisins** (en anglais \"k nearest neighbors\" : knn).\n\n\n### II. Le problème des iris\n\nAfin de travailler sur un exemple, nous allons utiliser un jeu de données relativement connu dans le monde du machine learning : le jeu de données \"iris\".\n\nEn 1936, Edgar Anderson a collecté des données sur 3 espèces d'iris : \"iris setosa\", \"iris virginica\" et \"iris versicolor\"\n\n<table align=\"left\">\n    <tbody>\n        <tr>\n            <td style=\"margin:10px; text-align:center\"><img src=\"https://dav74.github.io/site_nsi_prem/img/c32c_1.jpg\"></td>\n            <td style=\"margin:10px; text-align:center\"><img src=\"https://dav74.github.io/site_nsi_prem/img/c32c_2.jpg\"></td>\n            <td style=\"margin:10px; text-align:center\"><img src=\"https://dav74.github.io/site_nsi_prem/img/c32c_3.jpg\"></td>\n        </tr>\n        <tr>\n            <td style=\"text-align:center\">iris setosa</td>\n            <td style=\"text-align:center\">iris virginica</td>\n            <td style=\"text-align:center\">iris versicolor</td>\n        </tr>\n    </tbody>\n</table>\n\n<br style=\"clear:both\">\n\nPour chaque iris étudié, Anderson a mesuré (en cm) :\n- la largeur des sépales\n- la longueur des sépales\n- la largeur des pétales\n- la longueur des pétales\n\n**Par souci de simplification, nous nous intéresserons uniquement à la largeur et à la longueur des\npétales.**\n\nPour chaque iris mesuré, Anderson a aussi noté l'espèce (\"iris setosa\", \"iris versicolor\" ou \"iris virginica\")\n\nEn travaillant sur ce jeu de données, on peut obtenir le graphique suivant (en abscisse la longueur du pétale et en ordonnée la largeur du pétale) :\n\n<img src=\"https://dav74.github.io/site_nsi_prem/img/c32c_4.png\">\n<br style=\"clear:both\">\n\nNous obtenons des \"nuages\" de points, on remarque ces points sont regroupés par espèces d'iris (pour \"iris virginica\" et \"iris versicolor\", les points ont un peu tendance à se mélanger).\n\nImaginez maintenant qu'au cours d'une promenade vous trouviez un iris, n'étant pas un spécialiste, il ne vous est pas vraiment possible de déterminer l'espèce. En revanche, vous êtes capables de mesurer la longueur et la largeur des pétales de cet iris. Partons du principe qu'un pétale fasse 0,5 cm de large et 2 cm de long. Plaçons cette nouvelle donnée sur notre graphique (point en noir) :\n\n<img src=\"https://dav74.github.io/site_nsi_prem/img/c32c_5.png\">\n<br style=\"clear:both\">\n\nIl y a de fortes chances que votre iris soit de l'espèce \"iris setosa\".\n\n\n### III. Algorithme des k plus proches voisins\n\nIl est possible de rencontrer des cas plus difficiles, par exemple : largeur du pétale = 0,75 cm ; longueur du pétale = 2,5 cm :\n\n<img src=\"https://dav74.github.io/site_nsi_prem/img/c32c_6.png\">\n<br style=\"clear:both\">\n\nDans ce genre de cas, il peut être intéressant d'utiliser l'algorithme des \"k plus proches voisins\", mais en quoi consiste cet algorithme ?\n\n- on calcule la distance entre notre point (largeur du pétale = 0,75 cm ; longueur du pétale = 2,5 cm) et chaque point issu du jeu de données \"iris\" (à chaque fois c'est un calcul de distance entre 2 points)\n- on sélectionne uniquement les k distances les plus petites (les k plus proches voisins) \n- parmi les k plus proches voisins, on détermine quelle est l'espèce majoritaire. On associe à notre \"iris mystère\" cette espèce majoritaire parmi les k plus proches voisins.\n\nExemple avec **k = 3**\n<img src=\"https://dav74.github.io/site_nsi_prem/img/c32c_7.png\">\n<br style=\"clear:both\">\nLes 3 plus proches voisins sont signalés ci-dessus avec des flèches : nous avons deux \"iris setosa\" (point vert) et un \"iris versicolor\" (point rouge). D'après l'algorithme des \"k plus proches voisins\", notre \"iris mystère\" appartient à l'espèce \"setosa\".\n\nLe choix de la valeur de k est important, il faut souvent effectuer plusieurs essais.\n\n<br style=\"clear:both\">\n\n<span style=\"color:red; font-size:2em\">Activité 1</span>\n\nVous allez importer dans ce document le fichier \"iris.csv\" (disponible sur l'ENT).\n\nVous trouverez dans ce fichier :\n- la longueur des pétales\n- la largeur des pétales\n- l'espèce de l'iris (0 pour \"iris setosa\", 1 pour \"iris versicolor\" et 2 pour \"iris virginica\").\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas # importer la bibliothèque Pandas pour la manipulation des données\nimport matplotlib.pyplot as plt  # importer le module pyplot de matplotlib, renommé plt, qui permet de tracer des graphiques\n\niris=pandas.read_csv(\"iris.csv\") # stocker les données du CSV dans la variable iris\n\nx=iris.loc[:,\"petal_length\"] # la variable x contient la longueur des pétales\ny=iris.loc[:,\"petal_width\"] # la variable y contient la largeur des pétales\nlab=iris.loc[:,\"species\"] # la variable lab contient l'espèce de l'iris (0, 1 ou 2)\n\nplt.figure() #pour créer une nouvelle figure, automatique normalement mais bug sous basthon\nplt.axis('equal') # pour obtenir un repère orthonormé\nplt.scatter(x[lab == 0], y[lab == 0], color='g', label='setosa')\nplt.scatter(x[lab == 1], y[lab == 1], color='r', label='versicolor')\nplt.scatter(x[lab == 2], y[lab == 2], color='b', label='virginica')\n\n\"\"\"\nplt.scatter permet de tracer un lot de points. Pour la première ligne, pour tous les x et les y qui ont \"lab == 0\" (la condition est dans les crochets)\nEnsuite on défini la couleur, puis enfin l'étiquette (label) associé au groupe de point.\n\"\"\"\n\nplt.legend() #afficher la légende\nplt.show() #affiche le graphique","execution_count":9,"outputs":[{"output_type":"display_data","data":{"application/javascript":"element.append(window._basthonDomNodeBus.pop(0));"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"1  -  Ligne 3, sous quelle forme sont stockés les données du csv dans iris ? Donner le type et un exemple\n\nRéponse : \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"1  -  Ligne 4 et 5, sous quelle forme sont stockés les données des longueurs et largeurs dans les variables x et y ? Donner le type et un exemple\n\nRéponse : \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"<br>\n<hr>"},{"metadata":{},"cell_type":"markdown","source":"<span style=\"color:red; font-size:2em\">Activité 2</span>\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier #importer bibliothèque de Machine Learning\n\n#traitement CSV\niris=pandas.read_csv(\"iris.csv\")\nx=iris.loc[:,\"petal_length\"]\ny=iris.loc[:,\"petal_width\"]\nlab=iris.loc[:,\"species\"]\n#fin traitement CSV\n\n#valeurs\nlongueur=2.5\nlargeur=0.75\nk=3\n#fin valeurs\n\n#graphique\nplt.figure()\nplt.axis('equal')\nplt.scatter(x[lab == 0], y[lab == 0], color='g', label='setosa')\nplt.scatter(x[lab == 1], y[lab == 1], color='r', label='versicolor')\nplt.scatter(x[lab == 2], y[lab == 2], color='b', label='virginica')\nplt.scatter(longueur, largeur, color='k') # la k correspond à du noir -> CMJN = CMYK\nplt.legend()\n#fin graphique\n\n#algo knn\nd=list(zip(x,y)) #zip pour regrouper (Tuple), list pour donner le type list à d (tableau)\nmodel = KNeighborsClassifier(n_neighbors=k)\nmodel.fit(d,lab)\nprediction= model.predict([[longueur,largeur]])\n#fin algo knn\n\n#Affichage résultats\ntxt=\"Résultat : \"\nif prediction[0]==0:\n  txt=txt+\"setosa\"\nif prediction[0]==1:\n  txt=txt+\"versicolor\"\nif prediction[0]==2:\n  txt=txt+\"virginica\"\nplt.text(3,0.5, f\"largeur : {largeur} cm longueur : {longueur} cm\", fontsize=12)\nplt.text(3,0.3, f\"k : {k}\", fontsize=12)\nplt.text(3,0.1, txt, fontsize=12)\n#fin affichage résultats\n\nplt.show()","execution_count":15,"outputs":[{"output_type":"display_data","data":{"application/javascript":"element.append(window._basthonDomNodeBus.pop(0));"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"La bibliothèque Python Scikit Learn propose un grand nombre d'algorithmes lié au machine learning (c'est sans aucun doute la bibliothèque la plus utilisée en machine learning). Parmi tous ces algorithmes, Scikit Learn propose l'algorithme des k plus proches voisins.\n\nLe programme ci-dessus n'est pas très complexe à comprendre, nous allons tout de même nous attarder sur la partie \"knn\" :\n\n- **d=list(zip(x,y))**\n\n*permet de passer des 2 listes x et y :*\n<em>\n<span style=\"color:green\"><br>\nx = [1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, ...]<br>\ny = [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.4,....]<br>\n</span>\nà un tableau de tuples d :<br>\n<span style=\"color:green\">\nd = [(1.4, 0.2), (1.4, 0.2), (1.3, 0.2) (1.5, 0.2), (1.4, 0.2), (1.7, 0.2), (1.4, 0.4), ...]\n</span></em>\n\n- **model = KNeighborsClassifier(n_neighbors=k)**\n\n*défini le nombre de voisin à prendre en compte*\n\n- **model.fit(d,lab)**\n\n*permet d'associer les valeurs de \"d\" avec le lab correspondant.\nmême si ce sont 2 variables distinctes, elles sont encore classées dans le même ordre.*\n\n- **prediction= model.predict([[longueur,largeur]])**\n\n*Permet de récupérer une prédiction (sous forme d'un tableau d'une cellule) en fonction de la largeur et la longueur de l'iris à classer.*"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"Testez l'algorithme avec un k différent, que se passe t-il si vous choisissez un k à 5 ? pourquoi ?\n\nRéponse : \n\n"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"celltoolbar":"Format de la Cellule Texte Brut"},"nbformat":4,"nbformat_minor":2}